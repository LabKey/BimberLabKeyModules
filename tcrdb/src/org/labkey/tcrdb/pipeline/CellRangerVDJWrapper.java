package org.labkey.tcrdb.pipeline;

import au.com.bytecode.opencsv.CSVReader;
import au.com.bytecode.opencsv.CSVWriter;
import org.apache.commons.io.FileUtils;
import org.apache.commons.lang3.StringUtils;
import org.apache.log4j.Logger;
import org.jetbrains.annotations.Nullable;
import org.json.JSONObject;
import org.labkey.api.collections.CaseInsensitiveHashMap;
import org.labkey.api.data.ColumnInfo;
import org.labkey.api.data.CompareType;
import org.labkey.api.data.Container;
import org.labkey.api.data.ConvertHelper;
import org.labkey.api.data.DbSchema;
import org.labkey.api.data.DbSchemaType;
import org.labkey.api.data.SimpleFilter;
import org.labkey.api.data.Table;
import org.labkey.api.data.TableInfo;
import org.labkey.api.data.TableSelector;
import org.labkey.api.exp.api.ExpProtocol;
import org.labkey.api.exp.api.ExperimentService;
import org.labkey.api.iterator.CloseableIterator;
import org.labkey.api.laboratory.LaboratoryService;
import org.labkey.api.pipeline.PipelineJob;
import org.labkey.api.pipeline.PipelineJobException;
import org.labkey.api.query.FieldKey;
import org.labkey.api.query.QueryService;
import org.labkey.api.query.UserSchema;
import org.labkey.api.query.ValidationException;
import org.labkey.api.reader.FastaDataLoader;
import org.labkey.api.reader.FastaLoader;
import org.labkey.api.reader.Readers;
import org.labkey.api.sequenceanalysis.RefNtSequenceModel;
import org.labkey.api.sequenceanalysis.model.AnalysisModel;
import org.labkey.api.sequenceanalysis.model.ReadData;
import org.labkey.api.sequenceanalysis.model.Readset;
import org.labkey.api.sequenceanalysis.pipeline.AbstractAlignmentStepProvider;
import org.labkey.api.sequenceanalysis.pipeline.AlignerIndexUtil;
import org.labkey.api.sequenceanalysis.pipeline.AlignmentOutputImpl;
import org.labkey.api.sequenceanalysis.pipeline.AlignmentStep;
import org.labkey.api.sequenceanalysis.pipeline.CommandLineParam;
import org.labkey.api.sequenceanalysis.pipeline.IndexOutputImpl;
import org.labkey.api.sequenceanalysis.pipeline.PipelineContext;
import org.labkey.api.sequenceanalysis.pipeline.PipelineStepProvider;
import org.labkey.api.sequenceanalysis.pipeline.ReferenceGenome;
import org.labkey.api.sequenceanalysis.pipeline.SequenceAnalysisJobSupport;
import org.labkey.api.sequenceanalysis.pipeline.SequencePipelineService;
import org.labkey.api.sequenceanalysis.pipeline.ToolParameterDescriptor;
import org.labkey.api.sequenceanalysis.run.AbstractCommandPipelineStep;
import org.labkey.api.sequenceanalysis.run.AbstractCommandWrapper;
import org.labkey.api.sequenceanalysis.run.SimpleScriptWrapper;
import org.labkey.api.study.assay.AssayService;
import org.labkey.api.util.FileUtil;
import org.labkey.api.util.PageFlowUtil;
import org.labkey.api.view.ViewBackgroundInfo;
import org.labkey.api.view.ViewContext;
import org.labkey.api.writer.PrintWriters;
import org.labkey.tcrdb.TCRdbSchema;

import java.io.File;
import java.io.IOException;
import java.io.PrintWriter;
import java.nio.file.Files;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.Date;
import java.util.HashMap;
import java.util.HashSet;
import java.util.List;
import java.util.Map;
import java.util.Set;
import java.util.concurrent.atomic.AtomicBoolean;
import java.util.regex.Matcher;
import java.util.regex.Pattern;

public class CellRangerVDJWrapper extends AbstractCommandWrapper
{
    public CellRangerVDJWrapper(@Nullable Logger logger)
    {
        super(logger);
    }

    private static final String TARGET_ASSAY = "targetAssay";
    private static final String CELL_HASHING = "useCellHashing";
    private static final String READSET_TO_HASHING_MAP = "readsetToHashingMap";

    public static class VDJProvider extends AbstractAlignmentStepProvider<AlignmentStep>
    {
        public VDJProvider()
        {
            super("CellRanger VDJ", "Cell Ranger is an alignment/analysis pipeline specific to 10x genomic data, and this can only be used on fastqs generated by 10x.", Arrays.asList(
                    //--sample

                    ToolParameterDescriptor.create("id", "Run ID Suffix", "If provided, this will be appended to the ID of this run (readset name will be first).", "textfield", new JSONObject(){{
                        put("allowBlank", true);
                    }}, null),
                    ToolParameterDescriptor.createCommandLineParam(CommandLineParam.create("--force-cells"), "force-cells", "Force Cells", "Force pipeline to use this number of cells, bypassing the cell detection algorithm. Use this if the number of cells estimated by Cell Ranger is not consistent with the barcode rank plot.", "ldk-integerfield", new JSONObject(){{
                        put("minValue", 0);
                    }}, null),
                    ToolParameterDescriptor.create(CELL_HASHING, "Use Cell Hashing?", "If selected, this will use the data in the cDNA table to identify the proper HTOs and cell hashing readset.  It will run CiteSeqCount and generate HTO cells per cell", "checkbox", new JSONObject(){{
                        put("inputValue", true);
                    }}, false),
                    ToolParameterDescriptor.create(TARGET_ASSAY, "Target Assay", "Results will be loaded into this assay.  If no assay is selected, a table will be created with nothing in the DB.", "tcr-assayselectorfield", null, null)
            ), PageFlowUtil.set("tcrdb/field/AssaySelectorField.js"), "https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/what-is-cell-ranger", true, false, false, ALIGNMENT_MODE.MERGE_THEN_ALIGN);
        }

        public String getName()
        {
            return "CellRanger-VDJ";
        }

        public String getDescription()
        {
            return null;
        }

        public AlignmentStep create(PipelineContext context)
        {
            return new CellRangerVDJAlignmentStep(this, context, new CellRangerVDJWrapper(context.getLogger()));
        }
    }

    public static class CellRangerVDJAlignmentStep extends AbstractCommandPipelineStep<CellRangerVDJWrapper> implements AlignmentStep
    {
        public CellRangerVDJAlignmentStep(PipelineStepProvider provider, PipelineContext ctx, CellRangerVDJWrapper wrapper)
        {
            super(provider, ctx, wrapper);
        }

        @Override
        public boolean supportsMetrics()
        {
            return false;
        }

        @Override
        public void init(SequenceAnalysisJobSupport support) throws PipelineJobException
        {
            ReferenceGenome referenceGenome = support.getCachedGenomes().iterator().next();
            boolean hasCachedIndex = AlignerIndexUtil.hasCachedIndex(this.getPipelineCtx(), getIndexCachedDirName(getPipelineCtx().getJob()), referenceGenome);
            if (!hasCachedIndex)
            {
                getPipelineCtx().getLogger().info("Creating FASTA for CellRanger VDJ Index");
                File fasta = getGenomeFasta();
                try (PrintWriter writer = PrintWriters.getPrintWriter(fasta))
                {
                    UserSchema us = QueryService.get().getUserSchema(getPipelineCtx().getJob().getUser(), getPipelineCtx().getJob().getContainer(), "sequenceanalysis");
                    List<Integer> seqIds = new TableSelector(us.getTable("reference_library_members"), PageFlowUtil.set("ref_nt_id"), new SimpleFilter(FieldKey.fromString("library_id"), referenceGenome.getGenomeId()), null).getArrayList(Integer.class);
                    new TableSelector(us.getTable("ref_nt_sequences"), new SimpleFilter(FieldKey.fromString("rowid"), seqIds, CompareType.IN), null).forEach(nt -> {
                        //example: >1|TRAV41*01 TRAV41|TRAV41|L-REGION+V-REGION|TR|TRA|None|None
                        StringBuilder header = new StringBuilder();
                        header.append(">").append(nt.getRowid()).append("|").append(nt.getName()).append(" ").append(nt.getLineage()).append("|").append(nt.getLineage()).append("|");
                        //translate into V_Region
                        String type;
                        if (nt.getLineage().contains("J"))
                        {
                            type = "J-REGION";
                        }
                        else if (nt.getLineage().contains("V"))
                        {
                            type = "L-REGION+V-REGION";
                        }
                        else if (nt.getLineage().contains("C"))
                        {
                            type = "C-REGION";
                        }
                        else if (nt.getLineage().contains("D"))
                        {
                            type = "D-REGION";
                        }
                        else
                        {
                            throw new RuntimeException("Unknown lineage: " + nt.getLineage());
                        }

                        header.append(type).append("|TR|").append(nt.getLocus()).append("|None|None");

                        writer.write(header + "\n");
                        writer.write(nt.getSequence() + "\n");
                        nt.clearCachedSequence();

                    }, RefNtSequenceModel.class);
                }
                catch (IOException e)
                {
                    throw new PipelineJobException(e);
                }
            }

            if (getProvider().getParameterByName(CELL_HASHING).extractValue(getPipelineCtx().getJob(), getProvider(), getStepIdx(), Boolean.class, false))
            {
                getPipelineCtx().getLogger().debug("preparing cell hashing files");
                Container target = getPipelineCtx().getJob().getContainer().isWorkbook() ? getPipelineCtx().getJob().getContainer().getParent() : getPipelineCtx().getJob().getContainer();
                UserSchema tcr = QueryService.get().getUserSchema(getPipelineCtx().getJob().getUser(), target, TCRdbSchema.NAME);
                TableInfo cDNAs = tcr.getTable(TCRdbSchema.TABLE_CDNAS);

                Map<FieldKey, ColumnInfo> colMap = QueryService.get().getColumns(cDNAs, PageFlowUtil.set(
                        FieldKey.fromString("rowid"),
                        FieldKey.fromString("sortId/stimId/animalId"),
                        FieldKey.fromString("sortId/stimId/stim"),
                        FieldKey.fromString("sortId/population"),
                        FieldKey.fromString("sortId/hto"),
                        FieldKey.fromString("sortId/hto/sequence"),
                        FieldKey.fromString("hashingReadsetId"))
                );

                File output = getCDNAInfoFile();
                File barcodeOutput = getValidHashingBarcodeFile();
                HashMap<Integer, Integer> readsetToHashingMap = new HashMap<>();
                try (CSVWriter writer = new CSVWriter(PrintWriters.getPrintWriter(output), '\t', CSVWriter.NO_QUOTE_CHARACTER);CSVWriter bcWriter = new CSVWriter(PrintWriters.getPrintWriter(barcodeOutput), ',', CSVWriter.NO_QUOTE_CHARACTER))
                {
                    writer.writeNext(new String[]{"TCR_ReadsetId", "CDNA_ID", "AnimalId", "Stim", "Population", "HTO_Name", "HTO_Seq", "HashingReadsetId"});
                    List<Readset> cachedReadsets = support.getCachedReadsets();
                    Set<String> distinctHTOs = new HashSet<>();
                    for (Readset rs : cachedReadsets)
                    {
                        AtomicBoolean hasError = new AtomicBoolean(false);
                        //find cDNA records using this as enrichedReadset
                        new TableSelector(cDNAs, colMap.values(), new SimpleFilter(FieldKey.fromString("enrichedReadsetId"), rs.getRowId()), null).forEachResults(results -> {
                            writer.writeNext(new String[]{
                                    String.valueOf(rs.getRowId()),
                                    results.getString(FieldKey.fromString("rowid")),
                                    results.getString(FieldKey.fromString("sortId/stimId/animalId")),
                                    results.getString(FieldKey.fromString("sortId/stimId/stim")),
                                    results.getString(FieldKey.fromString("sortId/population")),
                                    results.getString(FieldKey.fromString("sortId/hto")),
                                    results.getString(FieldKey.fromString("sortId/hto/sequence")),
                                    String.valueOf(results.getInt(FieldKey.fromString("hashingReadsetId")))
                            });

                            if (results.getObject(FieldKey.fromString("hashingReadsetId")) == null)
                            {
                                hasError.set(true);
                            }

                            if (results.getObject(FieldKey.fromString("sortId/hto/sequence")) == null)
                            {
                                hasError.set(true);
                            }

                            support.cacheReadset(results.getInt(FieldKey.fromString("hashingReadsetId")), getPipelineCtx().getJob().getUser());
                            readsetToHashingMap.put(rs.getReadsetId(), results.getInt(FieldKey.fromString("hashingReadsetId")));

                            String hto = results.getString(FieldKey.fromString("sortId/hto")) + "<>" + results.getString(FieldKey.fromString("sortId/hto/sequence"));
                            if (!distinctHTOs.contains(hto) && !StringUtils.isEmpty(results.getString(FieldKey.fromString("sortId/hto/sequence"))))
                            {
                                distinctHTOs.add(hto);
                                bcWriter.writeNext(new String[]{results.getString(FieldKey.fromString("sortId/hto/sequence")), results.getString(FieldKey.fromString("sortId/hto"))});
                            }
                        });

                        if (hasError.get())
                        {
                            throw new PipelineJobException("No cell hashing readset or HTO found for one or more cDNAs. see the file: " + output.getName());
                        }
                    }

                    if (distinctHTOs.isEmpty())
                    {
                        throw new PipelineJobException("Cell hashing was selected, but no HTOs were found");
                    }

                    getPipelineCtx().getLogger().info("distinct HTOs: " + distinctHTOs.size());

                    support.cacheObject(READSET_TO_HASHING_MAP, readsetToHashingMap);
                }
                catch (IOException e)
                {
                    throw new PipelineJobException(e);
                }
            }
            else
            {
                getPipelineCtx().getLogger().debug("cell hashing was not selected");
            }
        }

        private File getGenomeFasta()
        {
            return new File(getPipelineCtx().getSourceDirectory(), "cellRangerVDJ.fasta");
        }

        @Override
        public String getIndexCachedDirName(PipelineJob job)
        {
            return getProvider().getName();
        }

        @Override
        public AlignmentStep.IndexOutput createIndex(ReferenceGenome referenceGenome, File outputDir) throws PipelineJobException
        {
            IndexOutputImpl output = new IndexOutputImpl(referenceGenome);

            File indexDir = new File(outputDir, getIndexCachedDirName(getPipelineCtx().getJob()));
            boolean hasCachedIndex = AlignerIndexUtil.hasCachedIndex(this.getPipelineCtx(), getIndexCachedDirName(getPipelineCtx().getJob()), referenceGenome);
            if (!hasCachedIndex)
            {
                getPipelineCtx().getLogger().info("Creating CellRanger VDJ Index");
                getPipelineCtx().getLogger().info("using file: " + getGenomeFasta().getPath());
                output.addIntermediateFile(getGenomeFasta());

                //remove if directory exists
                if (indexDir.exists())
                {
                    try
                    {
                        FileUtils.deleteDirectory(indexDir);
                    }
                    catch (IOException e)
                    {
                        throw new PipelineJobException(e);
                    }
                }

                output.addInput(getGenomeFasta(), "Input FASTA");

                List<String> args = new ArrayList<>();
                args.add(getWrapper().getExe().getPath());
                args.add("mkvdjref");
                args.add("--seqs=" + getGenomeFasta().getPath());
                args.add("--genome=" + indexDir.getName());

                getWrapper().setWorkingDir(indexDir.getParentFile());
                getWrapper().execute(args);

                output.appendOutputs(referenceGenome.getWorkingFastaFile(), indexDir);

                //recache if not already
                AlignerIndexUtil.saveCachedIndex(hasCachedIndex, getPipelineCtx(), indexDir, getIndexCachedDirName(getPipelineCtx().getJob()), referenceGenome);

            }

            return output;
        }

        @Override
        public AlignmentStep.AlignmentOutput performAlignment(Readset rs, File inputFastq1, @Nullable File inputFastq2, File outputDirectory, ReferenceGenome referenceGenome, String basename, String readGroupId, @Nullable String platformUnit) throws PipelineJobException
        {
            AlignmentOutputImpl output = new AlignmentOutputImpl();

            List<String> args = new ArrayList<>();
            args.add(getWrapper().getExe().getPath());
            args.add("vdj");

            String idParam = StringUtils.trimToNull(getProvider().getParameterByName("id").extractValue(getPipelineCtx().getJob(), getProvider(), getStepIdx(), String.class));
            String id = FileUtil.makeLegalName(rs.getName()) + (idParam == null ? "" : "-" + idParam);
            id = id.replaceAll("[^a-zA-z0-9_\\-]", "_");
            args.add("--id=" + id);

            File indexDir = AlignerIndexUtil.getWebserverIndexDir(referenceGenome, getIndexCachedDirName(getPipelineCtx().getJob()));
            args.add("--reference=" + indexDir.getPath());

            args.addAll(getClientCommandArgs("="));

            Integer maxThreads = SequencePipelineService.get().getMaxThreads(getPipelineCtx().getLogger());
            if (maxThreads != null)
            {
                args.add("--localcores=" + maxThreads.toString());
            }

            Integer maxRam = SequencePipelineService.get().getMaxRam();
            if (maxRam != null)
            {
                args.add("--localmem=" + maxRam.toString());
            }

            File localFqDir = new File(outputDirectory, "localFq");
            output.addIntermediateFile(localFqDir);
            Set<String> sampleNames = prepareFastqSymlinks(rs, localFqDir);
            args.add("--fastqs=" + localFqDir.getPath());

            getPipelineCtx().getLogger().debug("Sample names: [" + StringUtils.join(sampleNames, ",") + "]");
            if (sampleNames.size() > 1)
            {
                args.add("--sample=" + StringUtils.join(sampleNames, ","));
            }

            getWrapper().setWorkingDir(outputDirectory);
            getWrapper().execute(args);

            File outdir = new File(outputDirectory, id);
            outdir = new File(outdir, "outs");

            File bam = new File(outdir, "all_contig.bam");
            if (!bam.exists())
            {
                throw new PipelineJobException("Unable to find file: " + bam.getPath());
            }
            output.setBAM(bam);

            try
            {
                String prefix = FileUtil.makeLegalName(rs.getName() + "_");
                File outputHtml = new File(outdir, "web_summary.html");
                if (!outputHtml.exists())
                {
                    throw new PipelineJobException("Unable to find file: " + outputHtml.getPath());
                }

                File outputHtmlRename = new File(outdir, prefix + outputHtml.getName());
                if (outputHtmlRename.exists())
                {
                    outputHtmlRename.delete();
                }
                FileUtils.moveFile(outputHtml, outputHtmlRename);

                output.addSequenceOutput(outputHtmlRename, rs.getName() + " 10x VDJ Summary", "10x Run Summary", rs.getRowId(), null, referenceGenome.getGenomeId(), null);

                File outputVloupe = new File(outdir, "vloupe.vloupe");
                if (!outputVloupe.exists())
                {
                    throw new PipelineJobException("Unable to find file: " + outputVloupe.getPath());
                }

                File outputVloupeRename = new File(outdir, prefix + outputVloupe.getName());
                if (outputVloupeRename.exists())
                {
                    outputVloupeRename.delete();
                }
                FileUtils.moveFile(outputVloupe, outputVloupeRename);
                output.addSequenceOutput(outputVloupeRename, rs.getName() + " 10x VLoupe", "10x VLoupe", rs.getRowId(), null, referenceGenome.getGenomeId(), null);
            }
            catch (IOException e)
            {
                throw new PipelineJobException(e);
            }

            //NOTE: this folder has many unnecessary files and symlinks that get corrupted when we rename the main outputs
            File directory = new File(outdir.getParentFile(), "SC_VDJ_ASSEMBLER_CS");
            if (directory.exists())
            {
                //NOTE: this will have lots of symlinks, including corrupted ones, which java handles badly
                new SimpleScriptWrapper(getPipelineCtx().getLogger()).execute(Arrays.asList("rm", "-Rf", directory.getPath()));
            }
            else
            {
                getPipelineCtx().getLogger().warn("Unable to find folder: " + directory.getPath());
            }

            deleteSymlinks(localFqDir);

            if (getProvider().getParameterByName(CELL_HASHING).extractValue(getPipelineCtx().getJob(), getProvider(), getStepIdx(), Boolean.class, false))
            {
                runRemoteCellHashingTasks(output, getPerCellCsv(output.getBAM().getParentFile()), rs);
            }

            return output;
        }

        private File getCDNAInfoFile()
        {
            return new File(getPipelineCtx().getSourceDirectory(), "cDNAInfo.txt");
        }

        private File getValidHashingBarcodeFile()
        {
            return new File(getPipelineCtx().getSourceDirectory(), "validHashingBarcodes.csv");
        }

        private File getValidCellIndexFile()
        {
            return new File(getPipelineCtx().getSourceDirectory(), "validCellIndexes.csv");
        }

        private File runRemoteCellHashingTasks(AlignmentOutputImpl output, File perCellTsv, Readset rs) throws PipelineJobException
        {
            Map<Integer, Integer> readsetToHashing = getCachedReadsetMap(getPipelineCtx().getSequenceSupport());
            getPipelineCtx().getLogger().debug("total cashed readset/HTO pairs: " + readsetToHashing.size());

            //prepare whitelist of cell indexes
            File cellBarcodeWhitelist = getValidCellIndexFile();
            Set<String> uniqueBarcodes = new HashSet<>();
            getPipelineCtx().getLogger().debug("writing cell barcodes");
            try (CSVWriter writer = new CSVWriter(PrintWriters.getPrintWriter(cellBarcodeWhitelist), ',', CSVWriter.NO_QUOTE_CHARACTER);CSVReader reader = new CSVReader(Readers.getReader(perCellTsv), '\t'))
            {
                int rowIdx = 0;
                String[] row;
                while ((row = reader.readNext()) != null)
                {
                    //skip header
                    rowIdx++;
                    if (rowIdx > 1)
                    {
                        //NOTE: 10x appends "-1" to barcodes
                        String barcode = row[0].split("-")[0];
                        if (!uniqueBarcodes.contains(barcode))
                        {
                            writer.writeNext(new String[]{barcode});
                            uniqueBarcodes.add(barcode);
                        }
                    }
                }

                getPipelineCtx().getLogger().debug("rows inspected: " + (rowIdx - 1));
                getPipelineCtx().getLogger().debug("unique cell barcodes: " + uniqueBarcodes.size());
                output.addIntermediateFile(cellBarcodeWhitelist);
            }
            catch (IOException e)
            {
                throw new PipelineJobException(e);
            }

            //prepare whitelist of barcodes, based on cDNA records
            File htoBarcodeWhitelist = getValidHashingBarcodeFile();
            if (!htoBarcodeWhitelist.exists())
            {
                throw new PipelineJobException("Unable to find file: " + htoBarcodeWhitelist.getPath());
            }
            output.addIntermediateFile(htoBarcodeWhitelist);

            Readset htoReadset = getPipelineCtx().getSequenceSupport().getCachedReadset(readsetToHashing.get(rs.getReadsetId()));
            if (htoReadset == null)
            {
                throw new PipelineJobException("Unable to find HTO readset for readset: " + rs.getRowId());
            }

            //run CiteSeqCount.  this will use Multiseq to make calls per cell
            File cellToHto = getCellToHtoFile();
            File citeSeqCountUnknownOutput = new File(cellToHto.getParentFile(), "citeSeqUnknownBarcodes.txt");

            List<String> extraParams = new ArrayList<>();
            extraParams.add("-u");
            extraParams.add(citeSeqCountUnknownOutput.getPath());

            SequencePipelineService.get().runCiteSeqCount(htoReadset, htoBarcodeWhitelist, cellBarcodeWhitelist, cellToHto.getParentFile(), FileUtil.getBaseName(cellToHto.getName()), getPipelineCtx().getLogger(), extraParams);
            output.addOutput(cellToHto, "CiteSeqCount Counts");
            output.addOutput(citeSeqCountUnknownOutput, "CiteSeqCount Unknown Barcodes");

            if (citeSeqCountUnknownOutput.exists())
            {
                logTopUnknownBarcodes(citeSeqCountUnknownOutput, getPipelineCtx().getLogger());
            }

            return cellToHto;
        }

        private File getCellToHtoFile()
        {
            return new File(getPipelineCtx().getSourceDirectory(), "cellbarcodeToHTO.txt");
        }

        @Override
        public boolean doAddReadGroups()
        {
            return false;
        }

        @Override
        public boolean doSortIndexBam()
        {
            return false;
        }

        @Override
        public boolean alwaysCopyIndexToWorkingDir()
        {
            return false;
        }

        @Override
        public boolean supportsGzipFastqs()
        {
            return true;
        }

        private String getSymlinkFileName(String fileName)
        {
            //NOTE: cellranger is very picky about file name formatting
            Matcher m = FILE_PATTERN.matcher(fileName);
            if (m.matches())
            {
                if (!StringUtils.isEmpty(m.group(7)))
                {
                    return m.group(1).replaceAll("_", "-") + StringUtils.trimToEmpty(m.group(2)) + "_L" + StringUtils.trimToEmpty(m.group(3)) + "_" + StringUtils.trimToEmpty(m.group(4)) + StringUtils.trimToEmpty(m.group(5)) + StringUtils.trimToEmpty(m.group(6)) + ".fastq.gz";
                }
                else if (m.group(1).contains("_"))
                {
                    getPipelineCtx().getLogger().info("replacing underscores in file/sample name");
                    return m.group(1).replaceAll("_", "-") + StringUtils.trimToEmpty(m.group(2)) + "_L" + StringUtils.trimToEmpty(m.group(3)) + "_" + StringUtils.trimToEmpty(m.group(4)) + StringUtils.trimToEmpty(m.group(5)) + StringUtils.trimToEmpty(m.group(6)) + ".fastq.gz";
                }
                else
                {
                    getPipelineCtx().getLogger().info("no additional characters found");
                }
            }
            else
            {
                getPipelineCtx().getLogger().warn("filename does not match Illumina formatting: " + fileName);
            }

            return fileName;
        }

        public Set<String> prepareFastqSymlinks(Readset rs, File localFqDir) throws PipelineJobException
        {
            Set<String> ret = new HashSet<>();
            if (!localFqDir.exists())
            {
                localFqDir.mkdirs();
            }

            for (ReadData rd : rs.getReadData())
            {
                try
                {
                    File target1 = new File(localFqDir, getSymlinkFileName(rd.getFile1().getName()));
                    getPipelineCtx().getLogger().debug("file: " + rd.getFile1().getPath());
                    getPipelineCtx().getLogger().debug("target: " + target1.getPath());
                    if (target1.exists())
                    {
                        getPipelineCtx().getLogger().debug("deleting existing symlink: " + target1.getName());
                        Files.delete(target1.toPath());
                    }

                    Files.createSymbolicLink(target1.toPath(), rd.getFile1().toPath());
                    ret.add(getSampleName(target1.getName()));

                    if (rd.getFile2() != null)
                    {
                        File target2 = new File(localFqDir, getSymlinkFileName(rd.getFile2().getName()));
                        getPipelineCtx().getLogger().debug("file: " + rd.getFile2().getPath());
                        getPipelineCtx().getLogger().debug("target: " + target2.getPath());
                        if (target2.exists())
                        {
                            getPipelineCtx().getLogger().debug("deleting existing symlink: " + target2.getName());
                            Files.delete(target2.toPath());
                        }
                        Files.createSymbolicLink(target2.toPath(), rd.getFile2().toPath());
                        ret.add(getSampleName(target2.getName()));
                    }
                }
                catch (IOException e)
                {
                    throw new PipelineJobException(e);
                }
            }

            return ret;
        }

        public void deleteSymlinks(File localFqDir) throws PipelineJobException
        {
            for (File fq : localFqDir.listFiles())
            {
                try
                {
                    getPipelineCtx().getLogger().debug("deleting symlink: " + fq.getName());
                    Files.delete(fq.toPath());
                }
                catch (IOException e)
                {
                    throw new PipelineJobException(e);
                }
            }
        }

        private File getPerCellCsv(File outDir)
        {
            return new File(outDir, "all_contig_annotations.csv");
        }

        private void importAssayData(AnalysisModel model, File outDir) throws PipelineJobException
        {
            Integer assayId = getProvider().getParameterByName(TARGET_ASSAY).extractValue(getPipelineCtx().getJob(), getProvider(), getStepIdx(), Integer.class);
            if (assayId == null)
            {
                getPipelineCtx().getLogger().info("No assay selected, will not import");
                return;
            }

            ExpProtocol protocol = ExperimentService.get().getExpProtocol(assayId);
            if (protocol == null)
            {
                throw new PipelineJobException("Unable to find protocol: " + assayId);
            }

            File clonotypeCsv = new File(outDir, "clonotypes.csv");
            if (!clonotypeCsv .exists())
            {
                getPipelineCtx().getLogger().warn("unable to find consensus contigs: " + clonotypeCsv .getPath());
                return;
            }

            File allCsv = getPerCellCsv(outDir);
            if (!allCsv.exists())
            {
                getPipelineCtx().getLogger().warn("unable to find consensus contigs: " + allCsv .getPath());
                return;
            }

            File consensusCsv = new File(outDir, "consensus_annotations.csv");
            if (!consensusCsv .exists())
            {
                getPipelineCtx().getLogger().warn("unable to find consensus contigs: " + consensusCsv .getPath());
                return;
            }

            File consensusFasta = new File(outDir, "consensus.fasta");
            if (!consensusFasta.exists())
            {
                getPipelineCtx().getLogger().warn("unable to find FASTA: " + consensusFasta.getPath());
                return;
            }

            getPipelineCtx().getLogger().info("loading results into assay");

            Integer runId = SequencePipelineService.get().getExpRunIdForJob(getPipelineCtx().getJob());
            Readset rs = getPipelineCtx().getSequenceSupport().getCachedReadset(model.getReadset());

            //first build map of distinct FL sequences:
            getPipelineCtx().getLogger().info("processing FASTA: " + consensusFasta.getPath());
            Map<String, String> sequenceMap = new HashMap<>();
            try (FastaDataLoader loader = new FastaDataLoader(consensusFasta, false))
            {
                loader.setCharacterFilter(new FastaLoader.UpperAndLowercaseCharacterFilter());

                try (CloseableIterator<Map<String, Object>> i = loader.iterator())
                {
                    while (i.hasNext())
                    {
                        Map<String, Object> fastaRecord = i.next();
                        sequenceMap.put((String) fastaRecord.get("header"), (String) fastaRecord.get("sequence"));
                    }
                }
            }
            catch (IOException e)
            {
                throw new PipelineJobException(e);
            }

            getPipelineCtx().getLogger().info("total sequences: " + sequenceMap.size());

            File cDNAFile = getCDNAInfoFile();
            Map<String, CDNA> htoNameToCDNAMap = new HashMap<>();
            Map<Integer, CDNA> cDNAMap = new HashMap<>();
            if (cDNAFile.exists())
            {
                try (CSVReader reader = new CSVReader(Readers.getReader(cDNAFile), '\t'))
                {
                    String[] line;
                    while ((line = reader.readNext()) != null)
                    {
                        //header
                        if (line[0].startsWith("TCR_ReadsetId"))
                        {
                            continue;
                        }

                        CDNA cdna = CDNA.getRowId(Integer.parseInt(line[1]));
                        htoNameToCDNAMap.put(line[5], cdna);
                        cDNAMap.put(Integer.parseInt(line[1]), cdna);
                    }
                }
                catch (IOException e)
                {
                    throw new PipelineJobException(e);
                }
            }
            else
            {
                getPipelineCtx().getLogger().debug("Cell hashing is not used");
            }

            File cellbarcodeToHtoFile = getCellToHtoFile();
            Map<String, Integer> cellBarcodeToCDNAMap = new HashMap<>();
            if (cellbarcodeToHtoFile.exists())
            {
                try (CSVReader reader = new CSVReader(Readers.getReader(cellbarcodeToHtoFile), '\t'))
                {
                    //cellbarcode -> HTO name
                    String[] line;
                    while ((line = reader.readNext()) != null)
                    {
                        //header
                        if ("CellBarcode".equals(line[0]))
                        {
                            continue;
                        }

                        //NOTE: CiteSeqCounts concatenates name + HTO sequence.
                        String hto = line[1];
                        int i = hto.lastIndexOf("-");
                        if (i > -1)
                        {
                            hto = hto.substring(0, i);
                        }

                        CDNA cDNA = htoNameToCDNAMap.get(hto);
                        if (cDNA == null)
                        {
                            getPipelineCtx().getLogger().warn("Unable to find cDNA record for hto: " + hto);
                            continue;
                        }

                        cellBarcodeToCDNAMap.put(line[0], cDNA.getRowId());
                    }
                }
                catch (IOException e)
                {
                    throw new PipelineJobException(e);
                }
            }
            else
            {
                getPipelineCtx().getLogger().debug("Cell hashing is not used");
            }

            Map<String, Map<Integer, Integer>> countMapBySample = new HashMap<>();
            Map<Integer, Integer> totalCellsMapBySample = new HashMap<>();
            getPipelineCtx().getLogger().info("processing clonotype CSV: " + allCsv.getPath());

            boolean useCellHashing = getProvider().getParameterByName(CELL_HASHING).extractValue(getPipelineCtx().getJob(), getProvider(), getStepIdx(), Boolean.class, false);

            //header: barcode	is_cell	contig_id	high_confidence	length	chain	v_gene	d_gene	j_gene	c_gene	full_length	productive	cdr3	cdr3_nt	reads	umis	raw_clonotype_id	raw_consensus_id
            try (CSVReader reader = new CSVReader(Readers.getReader(allCsv), ','))
            {
                String[] line;
                int idx = 0;
                int totalSkipped = 0;
                Set<String> knownBarcodes = new HashSet<>();
                while ((line = reader.readNext()) != null)
                {
                    idx++;
                    if (idx == 1)
                    {
                        getPipelineCtx().getLogger().debug("skipping header, length: " + line.length);
                        continue;
                    }

                    if ("None".equals(line[12]))
                    {
                        continue;
                    }

                    //NOTE: 10x appends "-1" to barcode sequences
                    String barcode = line[0].split("-")[0];
                    Integer cDNA = useCellHashing ? cellBarcodeToCDNAMap.get(barcode) : -1;
                    if (cDNA == null)
                    {
                        getPipelineCtx().getLogger().info("skipping unknown barcode: " + barcode);
                        totalSkipped++;
                        continue;
                    }
                    knownBarcodes.add(barcode);

                    String clontypeId = removeNone(line[16]);
                    if (clontypeId != null)
                    {
                        Map<Integer, Integer> countMap = countMapBySample.getOrDefault(clontypeId, new HashMap<>());
                        countMap.put(cDNA, 1 + countMap.getOrDefault(cDNA, 0));
                        countMapBySample.put(clontypeId, countMap);

                        totalCellsMapBySample.put(cDNA, 1 + totalCellsMapBySample.getOrDefault(cDNA, 0));
                    }
                }

                getPipelineCtx().getLogger().info("total clonotype rows inspected: " + idx);
                getPipelineCtx().getLogger().info("total clonotype rows skipped for unknown barcodes: " + totalSkipped);
                getPipelineCtx().getLogger().info("unique known cell barcodes: " + knownBarcodes.size());
                getPipelineCtx().getLogger().info("total clonotypes: " + countMapBySample.size());
            }
            catch (IOException e)
            {
                throw new PipelineJobException(e);
            }

            //header for consensus_annotations.csv
            //clonotype_id	consensus_id	length	chain	v_gene	d_gene	j_gene	c_gene	full_length	productive	cdr3	cdr3_nt	reads	umis
            List<Map<String, Object>> rows = new ArrayList<>();
            getPipelineCtx().getLogger().info("processing consensus CSV: " + consensusCsv.getPath());
            try (CSVReader reader = new CSVReader(Readers.getReader(consensusCsv), ','))
            {
                String[] line;
                int idx = 0;
                while ((line = reader.readNext()) != null)
                {
                    idx++;
                    if (idx == 1)
                    {
                        getPipelineCtx().getLogger().debug("skipping header");
                        continue;
                    }

                    String cloneId = line[0];
                    Map<Integer, Integer> countData = countMapBySample.get(cloneId);
                    if (countData == null)
                    {
                        getPipelineCtx().getLogger().warn("No count data for clone: " + cloneId);
                        continue;
                    }

                    for (Integer cDNA : countData.keySet())
                    {
                        CDNA cDNARecord = cDNAMap.get(cDNA);

                        Map<String, Object> row = new CaseInsensitiveHashMap<>();

                        if (cDNA == -1)
                        {
                            row.put("sampleName", rs.getName());
                            row.put("subjectId", rs.getSubjectId());
                            row.put("sampleDate", rs.getSampleDate());
                        }
                        else if (cDNARecord == null)
                        {
                            throw new PipelineJobException("Unable to find cDNA for ID: " + cDNA);
                        }
                        else
                        {
                            row.put("sampleName", cDNARecord.getAssaySampleName());
                            row.put("subjectId", cDNARecord.getSortRecord().getStimRecord().getAnimalId());
                            row.put("sampleDate", cDNARecord.getSortRecord().getStimRecord().getDate());
                            row.put("cDNA", cDNA);
                        }

                        row.put("alignmentId", model.getAlignmentFile());
                        row.put("analysisId", model.getRowId());
                        row.put("pipelineRunId", runId);

                        row.put("cloneId", line[0]);
                        row.put("locus", line[3]);
                        row.put("vHit", removeNone(line[4]));
                        row.put("dHit", removeNone(line[5]));
                        row.put("jHit", removeNone(line[6]));
                        row.put("cHit", removeNone(line[7]));

                        row.put("cdr3", removeNone(line[10]));
                        row.put("cdr3_nt", removeNone(line[11]));

                        row.put("count", countData.get(cDNA));

                        double fraction = countData.get(cDNA).doubleValue() / totalCellsMapBySample.get(cDNA);
                        row.put("fraction", fraction);

                        if (!"None".equals(line[1]) && !sequenceMap.containsKey(line[1]))
                        {
                            getPipelineCtx().getLogger().warn("Unable to find sequence for: " + line[1]);
                        }
                        else
                        {
                            row.put("sequence", sequenceMap.get(line[1]));
                        }

                        rows.add(row);
                    }
                }
            }
            catch (Exception e)
            {
                getPipelineCtx().getLogger().error(e);
                throw new PipelineJobException(e);
            }

            getPipelineCtx().getLogger().info("total assay rows: " + rows.size());
            saveRun(protocol, model, rows, outDir);
        }

        private String removeNone(String input)
        {
            return "None".equals(input) ? null : input;
        }

        private void saveRun(ExpProtocol protocol, AnalysisModel model, List<Map<String, Object>> rows, File outDir) throws PipelineJobException
        {
            ViewBackgroundInfo info = getPipelineCtx().getJob().getInfo();
            ViewContext vc = ViewContext.getMockViewContext(info.getUser(), info.getContainer(), info.getURL(), false);

            JSONObject runProps = new JSONObject();
            runProps.put("performedby", getPipelineCtx().getJob().getUser().getDisplayName(getPipelineCtx().getJob().getUser()));
            runProps.put("assayName", "10x");
            runProps.put("Name", "Analysis: " + model.getAnalysisId());
            runProps.put("analysisId", model.getAnalysisId());

            Integer runId = SequencePipelineService.get().getExpRunIdForJob(getPipelineCtx().getJob());
            runProps.put("pipelineRunId", runId);

            JSONObject json = new JSONObject();
            json.put("Run", runProps);

            File assayTmp = new File(outDir, FileUtil.makeLegalName("10x-assay-upload_" + FileUtil.getTimestamp() + ".txt"));
            if (assayTmp.exists())
            {
                assayTmp.delete();
            }

            getPipelineCtx().getLogger().info("total rows imported: " + rows.size());
            if (!rows.isEmpty())
            {
                getPipelineCtx().getLogger().debug("saving assay file to: " + assayTmp.getPath());
                try
                {
                    LaboratoryService.get().saveAssayBatch(rows, json, assayTmp, vc, AssayService.get().getProvider(protocol), protocol);
                }
                catch (ValidationException e)
                {
                    throw new PipelineJobException(e);
                }
            }
        }

        public void addMetrics(AnalysisModel model) throws PipelineJobException
        {
            getPipelineCtx().getLogger().debug("adding 10x metrics");

            File metrics = new File(model.getAlignmentFileObject().getParentFile(), "metrics_summary.csv");
            if (metrics.exists())
            {
                try (CSVReader reader = new CSVReader(Readers.getReader(metrics)))
                {
                    String[] line;
                    String[] header = null;
                    String[] metricValues = null;

                    int i = 0;
                    while ((line = reader.readNext()) != null)
                    {
                        if (i == 0)
                        {
                            header = line;
                        }
                        else
                        {
                            metricValues = line;
                            break;
                        }

                        i++;
                    }

                    int totalAdded = 0;
                    TableInfo ti = DbSchema.get("sequenceanalysis", DbSchemaType.Module).getTable("quality_metrics");
                    for (int j = 0; j < header.length; j++)
                    {
                        Map<String, Object> toInsert = new CaseInsensitiveHashMap<>();
                        toInsert.put("container", getPipelineCtx().getJob().getContainer().getId());
                        toInsert.put("createdby", getPipelineCtx().getJob().getUser().getUserId());
                        toInsert.put("created", new Date());
                        toInsert.put("readset", model.getReadset());
                        toInsert.put("analysis_id", model.getRowId());
                        toInsert.put("dataid", model.getAlignmentFile());

                        toInsert.put("category", "Cell Ranger VDJ");
                        toInsert.put("metricname", header[j]);

                        metricValues[j] = metricValues[j].replaceAll(",", "");
                        Object val = metricValues[j];
                        if (metricValues[j].contains("%"))
                        {
                            metricValues[j] = metricValues[j].replaceAll("%", "");
                            Double d = ConvertHelper.convert(metricValues[j], Double.class);
                            d = d / 100.0;
                            val = d;
                        }

                        toInsert.put("metricvalue", val);

                        Table.insert(getPipelineCtx().getJob().getUser(), ti, toInsert);
                        totalAdded++;
                    }

                    getPipelineCtx().getLogger().info("total metrics added: " + totalAdded);
                }
                catch (IOException e)
                {
                    throw new PipelineJobException(e);
                }
            }
            else
            {
                getPipelineCtx().getLogger().warn("unable to find metrics file: " + metrics.getPath());
            }
        }

        public void complete(SequenceAnalysisJobSupport support, AnalysisModel model) throws PipelineJobException
        {
            addMetrics(model);

            File bam = model.getAlignmentData().getFile();
            if (bam.exists())
            {
                importAssayData(model, bam.getParentFile());
            }
            else
            {
                getPipelineCtx().getLogger().warn("BAM not found, expected: " + bam.getPath());
            }
        }

        private static Pattern FILE_PATTERN = Pattern.compile("^(.+?)(_S[0-9]+){0,1}_L(.+?)_(R){0,1}([0-9])(_[0-9]+){0,1}(.*?)(\\.f(ast){0,1}q)(\\.gz)?$");
        private static Pattern SAMPLE_PATTERN = Pattern.compile("^(.+)_S[0-9]+(.*)$");

        private String getSampleName(String fn)
        {
            Matcher matcher = FILE_PATTERN.matcher(fn);
            if (matcher.matches())
            {
                String ret = matcher.group(1);
                Matcher matcher2 = SAMPLE_PATTERN.matcher(ret);
                if (matcher2.matches())
                {
                    ret = matcher2.group(1);
                }
                else
                {
                    getPipelineCtx().getLogger().debug("_S not found in sample: [" + ret + "]");
                }

                ret = ret.replaceAll("_", "-");

                return ret;
            }
            else
            {
                getPipelineCtx().getLogger().debug("file does not match illumina pattern: [" + fn + "]");
            }

            throw new IllegalArgumentException("Unable to infer Illumina sample name: " + fn);
        }
    }

    //NOTE: jackson deserializes the map with string keys, so convert back to integer
    public static Map<Integer, Integer> getCachedReadsetMap(SequenceAnalysisJobSupport support) throws PipelineJobException
    {
        Map<Integer, Integer> ret = new HashMap<>();
        Map<String, Integer> cached = support.getCachedObject(READSET_TO_HASHING_MAP, PipelineJob.createObjectMapper().getTypeFactory().constructParametricType(Map.class, Integer.class, Integer.class));
        cached.forEach((x, y) -> ret.put(Integer.parseInt(x), y));

        return ret;
    }

    public static void logTopUnknownBarcodes(File citeSeqCountUnknownOutput, Logger log) throws PipelineJobException
    {
        try (CSVReader reader = new CSVReader(Readers.getReader(citeSeqCountUnknownOutput), ','))
        {
            String[] line;
            int lineIdx = 0;
            log.info("Top unknown barcodes:");
            while ((line = reader.readNext()) != null)
            {
                lineIdx++;
                if (lineIdx == 1)
                {
                    continue;
                }

                log.info(line[0] + ": " + line[1]);

                if (lineIdx == 7)
                {
                    break;
                }
            }
        }
        catch (IOException e)
        {
            throw new PipelineJobException(e);
        }
    }

    protected File getExe()
    {
        return SequencePipelineService.get().getExeForPackage("CELLRANGERPATH", "cellranger");
    }

    public static class CDNA
    {
        private int _rowId;
        private Integer _sortId;
        private String _chemistry;
        private Double _concentration;
        private String _plateId;
        private String _well;

        private Integer _readsetId;
        private Integer _enrichedReadsetId;
        private Integer _hashingReadsetId;
        private String _container;

        private Sort _sortRecord;

        public int getRowId()
        {
            return _rowId;
        }

        public void setRowId(int rowId)
        {
            _rowId = rowId;
        }

        public Integer getSortId()
        {
            return _sortId;
        }

        public void setSortId(Integer sortId)
        {
            _sortId = sortId;
        }

        public String getChemistry()
        {
            return _chemistry;
        }

        public void setChemistry(String chemistry)
        {
            _chemistry = chemistry;
        }

        public Double getConcentration()
        {
            return _concentration;
        }

        public void setConcentration(Double concentration)
        {
            _concentration = concentration;
        }

        public String getPlateId()
        {
            return _plateId;
        }

        public void setPlateId(String plateId)
        {
            _plateId = plateId;
        }

        public String getWell()
        {
            return _well;
        }

        public void setWell(String well)
        {
            _well = well;
        }

        public Integer getReadsetId()
        {
            return _readsetId;
        }

        public void setReadsetId(Integer readsetId)
        {
            _readsetId = readsetId;
        }

        public Integer getEnrichedReadsetId()
        {
            return _enrichedReadsetId;
        }

        public void setEnrichedReadsetId(Integer enrichedReadsetId)
        {
            _enrichedReadsetId = enrichedReadsetId;
        }

        public Integer getHashingReadsetId()
        {
            return _hashingReadsetId;
        }

        public void setHashingReadsetId(Integer hashingReadsetId)
        {
            _hashingReadsetId = hashingReadsetId;
        }

        public String getContainer()
        {
            return _container;
        }

        public void setContainer(String container)
        {
            _container = container;
        }

        public Sort getSortRecord()
        {
            if (_sortRecord == null)
            {
                _sortRecord = Sort.getRowId(_sortId);
            }

            return _sortRecord;
        }

        public String getAssaySampleName()
        {
            return getPlateId() + "_" + getWell() + "_" + getSortRecord().getStimRecord().getAnimalId() + "_" + getSortRecord().getStimRecord().getStim() + "_" + getSortRecord().getPopulation() + "_" + getSortRecord().getHto();
        }

        public static CDNA getRowId(int rowId)
        {
            return new TableSelector(TCRdbSchema.getInstance().getSchema().getTable(TCRdbSchema.TABLE_CDNAS)).getObject(rowId, CDNA.class);
        }
    }

    public static class Sort
    {
        private int _rowId;
        private Integer _stimId;
        private String _population;
        private String _hto;

        private Stim _stimRecord;

        public Stim getStimRecord()
        {
            if (_stimRecord == null)
            {
                _stimRecord = Stim.getRowId(_stimId);
            }

            return _stimRecord;
        }

        public int getRowId()
        {
            return _rowId;
        }

        public void setRowId(int rowId)
        {
            _rowId = rowId;
        }

        public Integer getStimId()
        {
            return _stimId;
        }

        public void setStimId(Integer stimId)
        {
            _stimId = stimId;
        }

        public String getPopulation()
        {
            return _population;
        }

        public void setPopulation(String population)
        {
            _population = population;
        }

        public String getHto()
        {
            return _hto;
        }

        public void setHto(String hto)
        {
            _hto = hto;
        }

        public void setStimRecord(Stim stimRecord)
        {
            _stimRecord = stimRecord;
        }

        public static Sort getRowId(int rowId)
        {
            return new TableSelector(TCRdbSchema.getInstance().getSchema().getTable(TCRdbSchema.TABLE_SORTS)).getObject(rowId, Sort.class);
        }
    }

    public static class Stim
    {
        private int _rowId;
        private String _animalId;
        private String _stim;
        private Date _date;

        public int getRowId()
        {
            return _rowId;
        }

        public void setRowId(int rowId)
        {
            _rowId = rowId;
        }

        public String getAnimalId()
        {
            return _animalId;
        }

        public void setAnimalId(String animalId)
        {
            _animalId = animalId;
        }

        public String getStim()
        {
            return _stim;
        }

        public void setStim(String stim)
        {
            _stim = stim;
        }

        public Date getDate()
        {
            return _date;
        }

        public void setDate(Date date)
        {
            _date = date;
        }

        public static Stim getRowId(int rowId)
        {
            return new TableSelector(TCRdbSchema.getInstance().getSchema().getTable(TCRdbSchema.TABLE_STIMS)).getObject(rowId, Stim.class);
        }
    }
}
